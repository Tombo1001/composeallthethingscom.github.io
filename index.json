[{"content":"In this blog post, we will guide you through the process of setting up and running Auto-GPT, a powerful, open source AI tool, using Docker. Auto-GPT is an experimental open-source application showcasing the capabilities of the GPT-4 language model. This program, driven by GPT-4, chains together LLM \u0026ldquo;thoughts\u0026rdquo;, to autonomously achieve whatever goal you set. As one of the first examples of GPT-4 running fully autonomously, Auto-GPT pushes the boundaries of what is possible with AI. By following these instructions, you will be able to quickly get started with Auto-GPT and leverage its capabilities. So let\u0026rsquo;s dive in!\nPrerequisites Before proceeding with the installation, make sure you have the following prerequisites:\nGit installed on your local machine. Docker and Docker Compose installed. For reference, I am running this all on a Ubuntun 20.04 install. If you are running on Windows or Mac, your milage may vary, but for the most parts these commands should be system agnostic.\nStep 1: Clone the Auto-GPT Repository To begin, open your terminal and execute the following command to clone the Auto-GPT repository from GitHub:\ngit clone https://github.com/Significant-Gravitas/Auto-GPT.git Local-Auto-GPT This command will create a local copy of the repository in a directory named \u0026ldquo;Local-Auto-GPT\u0026rdquo;.\nStep 2: Navigate to the Auto-GPT Directory and Copy Environment Template Next, navigate to the Auto-GPT directory using the cd command:\ncd Local-Auto-GPT Once inside the directory, execute the following command to create a copy of the environment template:\ncp .env.template .env This command will create a new .env file based on the provided template.\nStep 3: Set your OpenAI API Key To use Auto-GPT, you need to set your OpenAI API key. Open the .env file in a text editor and add the following line at the end of the file:\nOPENAI_API_KEY=$MY_OPENAI_API_KEY Replace $MY_OPENAI_API_KEY with your actual OpenAI API key. Make sure to save the changes to the .env file.\nStep 4: Build and Run Auto-GPT with Docker Compose Now, we are ready to build and run Auto-GPT using Docker Compose. Execute the following command:\ndocker-compose run --build --rm auto-gpt This command will build the necessary Docker image and start the Auto-GPT container. You should see the output and logs from the container in your terminal.\nCongratulations! You have successfully set up and launched Auto-GPT using Docker. Now you can start interacting with the model and explore its capabilities.\nConclusion: Use Cases for AI and Improving Efficiency Artificial Intelligence (AI) has become an integral part of our daily lives and has the potential to significantly improve efficiency and solve various problems. Here are a few use cases where AI, specifically largw language model (LLM) systems, could make a significant impact in the comes months-years:\nContent Generation: Large language models can generate high-quality content for various purposes, such as writing articles, blog posts, product descriptions, and social media captions. This can be a valuable tool for content creators and marketers to save time and maintain consistent output.\nCustomer Support Chatbots: AI-powered chatbots can understand and respond to customer inquiries, providing quick and accurate solutions. Large language models enable chatbots to handle complex queries, engage in natural conversations, and provide personalized recommendations, enhancing the customer support experience.\nAutomated Transcription and Summarization: Language models can transcribe audio or video content and generate summaries, saving time and effort in manual transcription tasks. This is particularly useful for journalists, researchers, and content creators who deal with large volumes of audiovisual content.\nCode Generation and Assistance: Large language models can assist developers by generating code snippets, providing code suggestions, and offering explanations for programming concepts. This can accelerate the development process, facilitate learning, and improve code quality.\nLanguage Tutoring and Learning: AI language models can act as virtual language tutors, helping learners practice conversations, providing instant feedback, and offering explanations of grammar rules and vocabulary. This personalized language learning experience can be highly effective and accessible to learners worldwide.\nCreative Writing and Storytelling: Large language models can assist writers in generating ideas, developing plotlines, and creating engaging narratives. They can serve as creative collaborators, offering inspiration, and helping writers overcome writer\u0026rsquo;s block.\nThese are just a few examples of how large language model driven AIs like Auto-GPT can be utilized to improve efficiency and solve various challenges across different domains. As the capabilities of AI continue to advance, we can expect even more innovative applications in the future.\n","permalink":"https://composeallthethings.com/setting-up-and-running-autogpt-with-docker-compose/","summary":"In this blog post, we will guide you through the process of setting up and running Auto-GPT, a powerful, open source AI tool, using Docker. Auto-GPT is an experimental open-source application showcasing the capabilities of the GPT-4 language model. This program, driven by GPT-4, chains together LLM \u0026ldquo;thoughts\u0026rdquo;, to autonomously achieve whatever goal you set. As one of the first examples of GPT-4 running fully autonomously, Auto-GPT pushes the boundaries of what is possible with AI.","title":"Setting up and Running AutoGPT With Docker Compose"},{"content":"Deploying a Prometheus and Grafana Monitoring Setup with Docker Compose If you\u0026rsquo;re looking to monitor your Raspberry Pi\u0026rsquo;s system performance, Prometheus and Grafana are a powerful duo that can help. And using Docker Compose makes setting up this monitoring system a breeze. In this blog post, we\u0026rsquo;ll walk through how to set up this monitoring system step-by-step using Docker Compose.\nThe Components There are three main components to this monitoring setup:\nRaspberry Pi running the Node Exporter Central Prometheus listener, which serves as the data source for Grafana Grafana Dashboard Setting Up the Raspberry Pi with Node Exporter First, we need to set up the Raspberry Pi with the Node Exporter. The Node Exporter is a Prometheus exporter that collects system metrics like CPU usage, memory usage, and disk I/O statistics.\nOne way to do this is to install Node Exporter directly on the Pi. Another way is to use a Docker container, which allows for easier management and distribution. In this example, we\u0026rsquo;ll be using the Docker container approach.\nHere\u0026rsquo;s an example docker-compose.yml file to set up the Node Exporter on the Pi:\nversion: \u0026#39;3\u0026#39; services: node-exporter: image: prom/node-exporter ports: - 9100:9100 networks: - monitoring networks: monitoring: This creates a node-exporter service running the Prometheus Node Exporter image, with the container\u0026rsquo;s port 9100 mapped to the host\u0026rsquo;s port 9100.\nCreating the Prometheus and Grafana Services Next, we need to set up the Prometheus and Grafana services using Docker Compose.\nversion: \u0026#39;3\u0026#39; services: node-exporter: image: prom/node-exporter ports: - 9100:9100 networks: - monitoring prometheus: image: prom/prometheus volumes: - ./prometheus.yml:/etc/prometheus/prometheus.yml ports: - 9090:9090 depends_on: - node-exporter networks: - monitoring grafana: image: grafana/grafana ports: - 3000:3000 depends_on: - prometheus networks: - monitoring networks: monitoring: This creates three services:\nThe node-exporter service running the Prometheus Node Exporter as we saw earlier. The prometheus service running the Prometheus Server. The prometheus.yml configuration file needs to be created manually. Here\u0026rsquo;s an example configuration file: global: scrape_interval: 15s scrape_configs: - job_name: \u0026#39;node-exporter\u0026#39; static_configs: - targets: [\u0026#39;node-exporter:9100\u0026#39;] This configuration instructs Prometheus to scrape the metrics exposed by the node-exporter service running on the node-exporter service name.\nThe grafana service running Grafana. Setting Up the Grafana Dashboard Finally, we need to set up the Grafana Dashboard to display the Pi\u0026rsquo;s CPU usage, disk usage, and network usage as a time series graph.\nHere\u0026rsquo;s an example dashboard configuration that you can import into Grafana:\napiVersion: 1 providers: - name: \u0026#39;Prometheus\u0026#39; orgId: 1 folder: \u0026#39;\u0026#39; type: \u0026#39;prometheus\u0026#39; access: \u0026#39;proxy\u0026#39; url: \u0026#39;http://prometheus:9090\u0026#39; basicAuth: false isDefault: true dashboard: id: null title: \u0026#39;Raspberry Pi Monitoring\u0026#39; hideControls: false editable: true graphTooltip: 0 rows: - title: \u0026#39;CPU Usage\u0026#39; height: \u0026#39;250px\u0026#39; panels: - title: \u0026#39;CPU Percentage\u0026#39; type: \u0026#39;graph\u0026#39; span: 12 fill: 1 linewidth: 2 targets: - expr: \u0026#39;100 - (avg by (instance) (irate(node_cpu_seconds_total{mode=\u0026#34;idle\u0026#34;}[1m])) * 100)\u0026#39; - title: \u0026#39;Disk Usage\u0026#39; height: \u0026#39;250px\u0026#39; panels: - title: \u0026#39;/mnt/data1\u0026#39; type: \u0026#39;graph\u0026#39; span: 12 fill: 1 linewidth: 2 targets: - expr: \u0026#39;node_filesystem_size{mountpoint=\u0026#34;/mnt/data1\u0026#34;} - node_filesystem_avail{mountpoint=\u0026#34;/mnt/data1\u0026#34;}\u0026#39; - title: \u0026#39;Network Usage\u0026#39; height: \u0026#39;250px\u0026#39; panels: - title: \u0026#39;Receive Bytes per Second\u0026#39; type: \u0026#39;graph\u0026#39; span: 6 fill: 1 linewidth: 2 targets: - expr: \u0026#39;rate(node_network_receive_bytes_total{device!~\u0026#34;lo\u0026#34;}[1m])\u0026#39; - title: \u0026#39;Transmit Bytes per Second\u0026#39; type: \u0026#39;graph\u0026#39; span: 6 fill: 1 linewidth: 2 targets: - expr: \u0026#39;rate(node_network_transmit_bytes_total{device!~\u0026#34;lo\u0026#34;}[1m])\u0026#39; schemaVersion: 16 version: 0 time: null timezone: \u0026#39;\u0026#39; This dashboard has three panels displaying CPU usage, disk usage, and network usage graphs.\nConclusion That\u0026rsquo;s it! You now have a Prometheus and Grafana monitoring setup to monitor your Raspberry Pi\u0026rsquo;s system performance. Using Docker Compose makes deploying this system quick and easy.\n","permalink":"https://composeallthethings.com/monitoring-a-raspberry-pi-with-prometheus-and-grafana/","summary":"Deploying a Prometheus and Grafana Monitoring Setup with Docker Compose If you\u0026rsquo;re looking to monitor your Raspberry Pi\u0026rsquo;s system performance, Prometheus and Grafana are a powerful duo that can help. And using Docker Compose makes setting up this monitoring system a breeze. In this blog post, we\u0026rsquo;ll walk through how to set up this monitoring system step-by-step using Docker Compose.\nThe Components There are three main components to this monitoring setup:","title":"Monitoring a Raspberry Pi with Prometheus and Grafana"},{"content":"Introduction to Running PostgreSQL in a Docker Container using Docker Compose In this blog post, we will explore how to run PostgreSQL in a Docker container using Docker Compose. We will also break down and explain the init-user-db.sh script that is executed at startup to initialize the PostgreSQL tables. Running PostgreSQL in a Docker container provides several benefits, including ease of deployment, portability, and isolation. So let\u0026rsquo;s dive in and understand the process!\nWhy Run PostgreSQL in a Docker Container? Running PostgreSQL in a Docker container offers numerous advantages. Here are a few key benefits:\nEasy Deployment: Docker simplifies the deployment process by encapsulating PostgreSQL and its dependencies into a container, making it easy to set up and manage.\nPortability: Docker containers are self-contained and can be run on any system that supports Docker, ensuring consistent behavior across different environments.\nIsolation: Running PostgreSQL in a container provides isolation from the host system, preventing potential conflicts with existing installations or dependencies.\nSetting Up PostgreSQL in a Docker Container with Docker Compose Before we start, ensure that you have Docker and Docker Compose installed on your system. Once both are set up, you can proceed with the following steps:\nCreate a Docker Compose file: Open a text editor and create a file called docker-compose.yml. Copy and paste the following contents into the file: version: \u0026#39;3.3\u0026#39; services: postgres: container_name: my-postgres image: postgres:latest restart: always environment: POSTGRES_USER: ${POSTGRES_USER} POSTGRES_PASSWORD: ${POSTGRES_PASSWORD} POSTGRES_DB: ${POSTGRES_DB} PGDATA: /var/lib/postgresql/data/pgdata ports: - \u0026#34;5432:5432\u0026#34; volumes: - ./data:/var/lib/postgresql/data/pgdata - ./init-user-db.sh:/docker-entrypoint-initdb.d/init-user-db.sh In this Docker Compose configuration:\nThe postgres service is defined with the specified container name (my-postgres), the latest PostgreSQL image (postgres:latest), and the restart policy set to always to ensure that the container restarts automatically if it stops.\nThe environment section sets the PostgreSQL environment variables (POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_DB) using the provided values from the environment. In this example I have a .env file in the same directory as the docker-compose.yml file, which means that there is no need for sensitive data such as passwords to be stored in our infrastructure code. This .env file could be replaced by git secrets in part of a pipeline deployment.\nThe ports section maps the host machine\u0026rsquo;s port 5432 to the container\u0026rsquo;s port 5432, allowing you to access the PostgreSQL database from your host machine.\nThe volumes section mounts two directories:\n./data:/var/lib/postgresql/data/pgdata: This maps the ./data directory on your host machine to the /var/lib/postgresql/data/pgdata directory inside the container. It allows you to persist the PostgreSQL data files across container restarts. ./init-user-db.sh:/docker-entrypoint-initdb.d/init-user-db.sh: This mounts the init-user-db.sh script from your host machine to the /docker-entrypoint-initdb.d/init-user-db.sh path inside the container. This script will be executed during container startup to initialize the PostgreSQL tables. Start the PostgreSQL container: Open your terminal or command prompt, navigate to the directory containing the docker-compose.yml file, and run the following command: docker-compose up -d The -d flag runs the containers in detached mode, allowing\nthem to run in the background.\nBreaking Down the init-user-db.sh Script The init-user-db.sh script is executed at startup to initialize the PostgreSQL tables. Let\u0026rsquo;s examine the script and understand its purpose:\n#!/bin/bash set -e set -u export PGDATABASE=${POSTGRES_DB} export PGUSER=${POSTGRES_USER} export PGPASSWORD=${POSTGRES_PASSWORD} RUN_PSQL=\u0026#34;psql -X --set AUTOCOMMIT=on --set ON_ERROR_STOP=on \u0026#34; ${RUN_PSQL} \u0026lt;\u0026lt;SQL CREATE TABLE public.mytable ( id uuid NOT NULL, sensor_name text NOT NULL, battery double precision, humidity double precision, link_quality double precision, temperature double precision, voltage integer, \u0026#34;time\u0026#34; timestamp with time zone NOT NULL ) TABLESPACE pg_default; ALTER TABLE IF EXISTS public.mytable OWNER to dbadm; SQL Here\u0026rsquo;s a breakdown of the script:\nThe set -e and set -u commands ensure that the script exits immediately if any command fails or encounters an unset variable.\nThe export statements set environment variables to configure the PostgreSQL connection. The variables PGDATABASE, PGUSER, and PGPASSWORD are assigned values based on the corresponding Docker environment variables (derived from the .env file).\nThe RUN_PSQL variable defines the psql command with specific options. -X disables transaction management, --set AUTOCOMMIT=on ensures that each command is executed in its own transaction, and --set ON_ERROR_STOP=on stops the script execution if any error occurs.\nThe ${RUN_PSQL} \u0026lt;\u0026lt;SQL syntax starts a here-document that allows us to provide SQL commands inline.\nThe SQL commands enclosed within the SQL delimiter create a table named mytable in the public schema with the specified columns and data types. The ALTER TABLE statement sets the owner of the table to dbadm if the table already exists.\nWith this updated Docker Compose configuration, you can easily manage and deploy your PostgreSQL container along with the initialization script. Running docker-compose up -d will start the containers and execute the init-user-db.sh script to initialize the PostgreSQL tables.\nConclusion In this blog post, we have learned how to run PostgreSQL in a Docker container using Docker Compose. We have seen the advantages of running PostgreSQL in a container and how to set it up with the docker-compose.yml file. Additionally, we have explored the init-user-db.sh script and its role in initializing the PostgreSQL tables.\nBy leveraging Docker and Docker Compose, you can easily deploy and manage PostgreSQL in a portable and isolated environment. This approach brings flexibility and scalability to your PostgreSQL deployments, making it an ideal choice for various applications.\nHopefully this blog post has been helpful in understanding the process of running PostgreSQL in a Docker container using Docker Compose. Stay tuned for more exciting tutorials and guides. Consider buying me a coffee to feed the addiction and help me solve more automation headaches!\n","permalink":"https://composeallthethings.com/automating-postgres-deployment-with-docker-compose-and-init-scripts/","summary":"Introduction to Running PostgreSQL in a Docker Container using Docker Compose In this blog post, we will explore how to run PostgreSQL in a Docker container using Docker Compose. We will also break down and explain the init-user-db.sh script that is executed at startup to initialize the PostgreSQL tables. Running PostgreSQL in a Docker container provides several benefits, including ease of deployment, portability, and isolation. So let\u0026rsquo;s dive in and understand the process!","title":"Automating Postgres Deployment with Docker Compose and Init Scripts"},{"content":"Welcome to my New Blog: # Welcome to my New Blog: Exploring Docker Compose, Orchestration, and Systems Development! Hey there! I\u0026rsquo;m excited to introduce you to my brand-new blog, a dedicated space where I\u0026rsquo;ll be showcasing all of my Docker Compose, Orchestration, and Systems Development projects. I created this blog because my previous one, exitcode0.net, was starting to get cluttered and lose focus. With this fresh start, I aim to provide you with valuable content and stay on topic.\nHelping You Replicate My Systems One of my primary goals for this blog is to offer as much help as possible to those looking to replicate the systems I\u0026rsquo;m building. I understand how valuable it is to have practical resources when diving into new projects, especially in the world of systems orchestration and DevOps.\nIn each blog post, I\u0026rsquo;ll walk you through the entire process, sharing detailed steps, explanations, and insights into the systems I\u0026rsquo;m working on. Whether you\u0026rsquo;re a beginner or an experienced professional, my aim is to make the content accessible and easy to follow, so you can replicate the configurations and workflows I\u0026rsquo;m showcasing.\nMotivations: Self-learning and Documentation Why am I recording and sharing my projects? Well, there are a couple of key motivations behind this endeavor. Firstly, by documenting my projects, I solidify my own understanding and knowledge. As I explain the processes and configurations, it deepens my grasp of the concepts involved.\nSecondly, this blog serves as a means of documentation for myself. Should I need to refer back to any specific configurations or workings in the future, I can simply access my blog and find what I\u0026rsquo;m looking for. It\u0026rsquo;s an invaluable resource for personal growth and professional development.\nA Resource for Breaking into Systems Orchestration and DevOps While creating this blog primarily serves my own learning and documentation purposes, I sincerely hope that it becomes a useful resource for anyone looking to break into the fascinating world of systems orchestration and DevOps. I understand how challenging it can be to get started and navigate the complexities of these domains. That\u0026rsquo;s why I want to share my knowledge and experiences to help make your journey smoother.\nWhether you\u0026rsquo;re struggling with Docker Compose, intrigued by orchestration techniques, or curious about systems development, you\u0026rsquo;ve come to the right place. I\u0026rsquo;ll do my best to explain concepts, provide practical examples, and guide you through the intricacies of these topics.\nJoin Me on this Exciting Journey! I\u0026rsquo;m thrilled to embark on this new blogging adventure, and I\u0026rsquo;m eager to have you join me along the way. Together, we\u0026rsquo;ll explore Docker Compose, delve into the world of orchestration, and dive deep into systems development. I\u0026rsquo;ll be sharing my projects, insights, and valuable resources to empower you on your own path.\nMake sure to stay tuned for upcoming posts where we\u0026rsquo;ll dive right into the action. Whether you\u0026rsquo;re a beginner or an experienced practitioner, this blog aims to be your go-to resource for all things Docker Compose, Orchestration, and Systems Development.\nLet\u0026rsquo;s learn, build, and grow together!!\nHey there! I\u0026rsquo;m excited to introduce you to my brand-new blog, a dedicated space where I\u0026rsquo;ll be showcasing all of my Docker Compose, Orchestration, and Systems Development projects. I created this blog because my previous one, exitcode0.net, was starting to get cluttered and lose focus. With this fresh start, I aim to provide you with valuable content and stay on topic.\nHelping You Replicate My Systems One of my primary goals for this blog is to offer as much help as possible to those looking to replicate the systems I\u0026rsquo;m building. I understand how valuable it is to have practical resources when diving into new projects, especially in the world of systems orchestration and DevOps.\nIn each blog post, I\u0026rsquo;ll walk you through the entire process, sharing detailed steps, explanations, and insights into the systems I\u0026rsquo;m working on. Whether you\u0026rsquo;re a beginner or an experienced professional, my aim is to make the content accessible and easy to follow, so you can replicate the configurations and workflows I\u0026rsquo;m showcasing.\nMotivations: Self-learning and Documentation Why am I recording and sharing my projects? Well, there are a couple of key motivations behind this endeavor. Firstly, by documenting my projects, I solidify my own understanding and knowledge. As I explain the processes and configurations, it deepens my grasp of the concepts involved.\nSecondly, this blog serves as a means of documentation for myself. Should I need to refer back to any specific configurations or workings in the future, I can simply access my blog and find what I\u0026rsquo;m looking for. It\u0026rsquo;s an invaluable resource for personal growth and professional development.\nA Resource for Breaking into Systems Orchestration and DevOps While creating this blog primarily serves my own learning and documentation purposes, I sincerely hope that it becomes a useful resource for anyone looking to break into the fascinating world of systems orchestration and DevOps. I understand how challenging it can be to get started and navigate the complexities of these domains. That\u0026rsquo;s why I want to share my knowledge and experiences to help make your journey smoother.\nWhether you\u0026rsquo;re struggling with Docker Compose, intrigued by orchestration techniques, or curious about systems development, you\u0026rsquo;ve come to the right place. I\u0026rsquo;ll do my best to explain concepts, provide practical examples, and guide you through the intricacies of these topics.\nJoin Me on this Exciting Journey! I\u0026rsquo;m thrilled to embark on this new blogging adventure, and I\u0026rsquo;m eager to have you join me along the way. Together, we\u0026rsquo;ll explore Docker Compose, delve into the world of orchestration, and dive deep into systems development. I\u0026rsquo;ll be sharing my projects, insights, and valuable resources to empower you on your own path.\nMake sure to stay tuned for upcoming posts where we\u0026rsquo;ll dive right into the action. Whether you\u0026rsquo;re a beginner or an experienced practitioner, this blog aims to be your go-to resource for all things Docker Compose, Orchestration, and Systems Development.\nLet\u0026rsquo;s learn, build, and grow together!\n","permalink":"https://composeallthethings.com/a-new-beginning-let-us-explore/","summary":"Welcome to my New Blog: # Welcome to my New Blog: Exploring Docker Compose, Orchestration, and Systems Development! Hey there! I\u0026rsquo;m excited to introduce you to my brand-new blog, a dedicated space where I\u0026rsquo;ll be showcasing all of my Docker Compose, Orchestration, and Systems Development projects. I created this blog because my previous one, exitcode0.net, was starting to get cluttered and lose focus. With this fresh start, I aim to provide you with valuable content and stay on topic.","title":"A New Beginning - Let us explore!"}]